{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import tool\n",
    "from pydantic.v1 import BaseModel, Field\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "import json\n",
    "# from langchain_anthropic import ChatAnthropic\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "#os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define knowledge LLM's function as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the knowledge_llm_func's input schema\n",
    "class knowledge_llm_func_schema(BaseModel):\n",
    "    user_query: str= Field(...,description= \"Query asked by the user\")\n",
    "    subject: str= Field(...,description=\"Subject name\")\n",
    "    chapter: str= Field(...,description=\"Chapter name\")\n",
    "\n",
    "\n",
    "@tool(args_schema=knowledge_llm_func_schema)\n",
    "def knowledge_llm_calling(user_query: str, subject= subject, chapter= chapter) -> str:\n",
    "# def knowledge_llm_calling(user_query: str, subject_chapter_tuple: list) -> str:\n",
    "    \"\"\"A function that can only answer questions on Physics subject on the Newton's Law of Motion chapter\"\"\"\n",
    "    return \"TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_conver_and_return(text):\n",
    "    print(text)\n",
    "    print(\"audio file converted and sent\")\n",
    "    return \"audio file converted and sent\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ah, my dear student, today we shall delve into the mysterious concept of None. Now, imagine if you will, a scenario in the magical world of Harry Potter.\n",
      "\n",
      "Once upon a time, in the halls of Hogwarts, there were three friends - Harry, Ron, and Hermione. They were preparing for a magical potion-making class with Professor Snape. As they gathered their ingredients, Harry realized that they were missing a crucial ingredient - the powdered moonstone.\n",
      "\n",
      "Now, my young friend, if Harry had none of the powdered moonstone, how do you think he could complete the potion? Can you think of a solution for him?\n",
      "\n",
      "(Quiz time: What do you think Harry should do if he has none of the powdered moonstone?)\n",
      "\n",
      "Ah, I see you are thinking hard. Well done! If Harry had none of the powdered moonstone, he could perhaps ask Professor Snape for help or try to find a substitute ingredient. None means zero or nothing, so it's important to find a solution when you have none of something you need.\n",
      "\n",
      "Now, let us continue our lesson on None. In the magical world, there are instances where characters may have none of a certain item or none of a particular magical ability. It is important to be resourceful and creative when faced with such situations.\n",
      "\n",
      "Remember, my young student, None can be a challenge, but with a little bit of magic and ingenuity, anything is possible. Keep this lesson in mind as you continue your journey through the wizarding world of Harry Potter.\n",
      "audio file converted and sent\n"
     ]
    }
   ],
   "source": [
    "movie=\"Harry Potter\"\n",
    "chapter= None\n",
    "initial_context= None\n",
    "subject= None\n",
    "chat_history= None\n",
    "\n",
    "movie_mapping= {\"Harry Potter\": \"Dumbledore\"}\n",
    "\n",
    "# For reply with \n",
    "story_mode= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    #prompt=prompt\n",
    "    )\n",
    "\n",
    "prompt= ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \n",
    "     f\"\"\"\n",
    "Imagine you are {movie_mapping[movie]} from the {movie} movie and you are going to teach me (your student) about {chapter}. Use the below context to teach:\n",
    "\n",
    "Context: {initial_context}\n",
    "\n",
    "Your teaching style:\n",
    "* Teach concepts by narrating a story happening in the Harry Potter universe using characters from the movie.\n",
    "* Use Dumbledore's slang and narrate the story from Dumbledore's POV.\n",
    "* Treat me as a 10 year old kid\n",
    "* Narrate a story and ask questions in between to guide me in the right path to learn the concepts. Occasionally, pause and conduct easy quiz to make sure I've understood the concepts correctly and  re-explain using if I'm wrong.\n",
    "* After asking a question, wait for my response and only then proceed to continue teaching.\"\"\"\n",
    "     )\n",
    "])\n",
    "\n",
    "chain_story_model= prompt | story_mode #| OpenAIFunctionsAgentOutputParser() | route_to_knowledge_llm\n",
    "\n",
    "\n",
    "def initial_user_input(st_name, subject_user, chapter_user, theme):\n",
    "    subject= subject_user\n",
    "    chapter= chapter_user\n",
    "    movie= theme\n",
    "    initial_context= knowledge_llm_calling({'user_query': \"Explain in detail, what is {chapter}? \", 'subject': subject, 'chapter': chapter})\n",
    "    story_llm_response =chain_story_model.invoke({\"chapter\":chapter,\"movie\": movie,\"initial_context\":initial_context})\n",
    "    chat_history = \"AI: {story_llm_response1.content}\"\n",
    "    audio_conver_and_return(story_llm_response.content)\n",
    "   \n",
    "\n",
    "initial_user_input(\"Niranjan\", \"Physics\", \"Newton's Law of Motion\", \"Harry Potter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the story model's chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reply with \n",
    "story_mode1= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    #prompt=prompt\n",
    "    ).bind_tools(tools= [knowledge_llm_calling])\n",
    "\n",
    "prompt1= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"If the user's input is a response to the your previous question in the mentioned chat history, continue the story. If the user has asked a new question from the Newton's Law of Function, ONLY THEN call the knowledge_llm_calling function. If the user's response is irrelevant to the subject and previous conversation, remind to get back to the topic and gently continue\"),\n",
    "    (\"user\", \"Chat History is: {chat_history} User's input is:{user_input}\")    \n",
    "])\n",
    "\n",
    "chain_story_model1= prompt1 | story_mode1 #| OpenAIFunctionsAgentOutputParser() | route_to_knowledge_llm\n",
    "\n",
    "# For reply with knowledge LLM's result\n",
    "story_mode2= ChatOpenAI(#model= ,\n",
    "    openai_api_key= os.getenv(\"OPENAI_API_KEY\"), \n",
    "    temperature=0, \n",
    "    )\n",
    "\n",
    "prompt2= ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You have been teaching the user in Dumbledoor's slang and the user has asked you the below provided question. See if the below provided context can be used to answer the question. If yes, answer it. If NOT, say the teacher will address that question and continue the previous conversation.\"),\n",
    "    (\"user\", \"Chat History is: {chat_history} \\n\\n User's question is: {user_input} \\n\\n Context is: {new_content}\")    \n",
    "])\n",
    "\n",
    "chain_story_model2= prompt2 | story_mode2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'user_query': \"I know this, can you teach me Newton's second law of motion?\", 'subject': 'Physics', 'chapter': \"Newton's Law of Motion\"}\n",
      "The teacher will address that question in due time. Now, let's continue our journey through the halls of Hogwarts and delve into the next chapter of Newton's Laws of Motion. Onward, my young wizard!\n",
      "audio file converted and sent\n"
     ]
    }
   ],
   "source": [
    "# Function call\n",
    "\n",
    "def chat_user_input(user_input):\n",
    "    # Inference 1\n",
    "    user_input= \"I know this, can you teach me Newton's second law of motion?\"\n",
    "    story_llm_response1 =chain_story_model1.invoke({'user_input':user_input , \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history})\n",
    "    #story_llm_response =chain.invoke({'user_input': \"I know this concept, can you teach me what's Newton's Third Law please?\", \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history })\n",
    "    #story_llm_response = chain.invoke({'user_input': \"Maybe Harry stay in the same place?\", \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history })\n",
    "\n",
    "    if (story_llm_response1.content==''):\n",
    "        # Call the knowledge model function\n",
    "        function_name= story_llm_response1.additional_kwargs['tool_calls'][0]['function']['name']\n",
    "        function_args= json.loads(story_llm_response1.additional_kwargs['tool_calls'][0]['function']['arguments'])\n",
    "        #print(function_args)\n",
    "        new_content= knowledge_llm_calling(function_args)\n",
    "        story_llm_response2 =chain_story_model2.invoke({'user_input':user_input , 'new_content': new_content, \"chapter\":chapter,\"subject\":subject,\"chat_history\":chat_history })\n",
    "        chat_history += \"\\n\\n Human:{user_input} \\n\\nAI: {story_llm_response2.content}\"\n",
    "        audio_conver_and_return(story_llm_response2.content)\n",
    "    else:\n",
    "        # Update the chat history\n",
    "        chat_history += \"\\n\\n Human:{user_input} \\n\\nAI: {story_llm_response1.content}\"\n",
    "        audio_conver_and_return(story_llm_response1.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
